{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf9faad",
   "metadata": {},
   "source": [
    "# Data Mining - StarCraft Player Prediction\n",
    "\n",
    "The goal of the project was it to predict which player is playing a certain game based on the moves he or she made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from itertools import groupby, combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cbede",
   "metadata": {},
   "source": [
    "## Pre-processing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900248a1",
   "metadata": {},
   "source": [
    "To train a model we were given data where each row represented a gameplay. A row included information about the race, the player who was playing and the moves he or she made during the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv('train_data.csv', delimiter=';')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data = train_data.drop(['PlayerURL', 'PlayerName'], axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6becb",
   "metadata": {},
   "source": [
    "Before starting with the classification algorithms we had to find a way to process our data and make it comparable. We wanted to reduce dimensions without losing information. In order to do so, we started counting how often a player would use a certain move. We did this over the course of a whole game and also over the course of certain time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_moves(row, counts, index):\n",
    "    total_moves = 0\n",
    "    for i in range(1, 2564):\n",
    "        move = row[\"Move \"+ str(i)]\n",
    "        # count the number of s's\n",
    "        if move == 's':\n",
    "            counts[10][index] += 1\n",
    "        # count the number of Base's\n",
    "        elif move == 'Base':\n",
    "            counts[11][index] += 1\n",
    "        # count the number of SingleMineral's\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[12][index] += 1\n",
    "        # count the hotkeys\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[j][index] += 1\n",
    "\n",
    "        total_moves += 1  \n",
    "    # Save the total moves count\n",
    "    counts[13][index] = total_moves\n",
    "\n",
    "\n",
    "def count_move_per_time(row, counts, row_index, time_interval, ti_index):\n",
    "    base_index = ti_index * 14\n",
    "    total_moves = 0\n",
    "\n",
    "    for i in range(1, 2564):\n",
    "        move = row[\"Move \" + str(i)]\n",
    "\n",
    "        # Count actions for the given time interval\n",
    "        if move == 's':\n",
    "            counts[base_index + 10][row_index] += 1\n",
    "        elif move == 'Base':\n",
    "            counts[base_index + 11][row_index] += 1\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[base_index + 12][row_index] += 1\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[base_index + j][row_index] += 1\n",
    "\n",
    "        total_moves += 1\n",
    "\n",
    "        # Counting actions after the specified time interval\n",
    "        if move == f't{time_interval}':\n",
    "            break\n",
    "\n",
    "    counts[base_index + 13][row_index] = total_moves\n",
    "\n",
    "# Create new table that only contains the first column (PlayerId) of train_data\n",
    "# Keep only the first column but all rows\n",
    "train_data_new = train_data.iloc[:, :1]\n",
    "\n",
    "# Specify the target time intervals\n",
    "# time_intervals = [20, 60, 100, 200]\n",
    "time_intervals = [5, 20, 60, 100, 200, 270, 340, 550]\n",
    "\n",
    "# Calculate how many columns we need\n",
    "calc_column = len(time_intervals)* 14 + 14\n",
    "\n",
    "# New lists of counts\n",
    "counts = [[0] * 3052 for _ in range(calc_column)]\n",
    "\n",
    "# Go through the rows using the functions to count the actions\n",
    "for row_index, row in train_data.iterrows():\n",
    "    count_moves(row, counts, row_index)\n",
    "\n",
    "    for ti_index, time_interval in enumerate(time_intervals):\n",
    "        count_move_per_time(row, counts, row_index, time_interval, ti_index+1)\n",
    "\n",
    "train_data_new[\"sCount\"] = counts[10]\n",
    "train_data_new[\"BaseCount\"] = counts[11]\n",
    "train_data_new[\"singleMineralCount\"] = counts[12]\n",
    "\n",
    "for i in range(10):\n",
    "    train_data_new[f'hk{i}Count'] = counts[i]\n",
    "\n",
    "for ti_index, time_interval in enumerate(time_intervals):\n",
    "    base_index = (ti_index + 1) * 14\n",
    "    \n",
    "    train_data_new[f's_t{time_interval}Count'] = counts[base_index + 10]\n",
    "    train_data_new[f'base_t{time_interval}Count'] =  counts[base_index + 11]\n",
    "    train_data_new[f'singleMineral_t{time_interval}Count'] = counts[base_index + 12]\n",
    "    \n",
    "    for j in range(10):\n",
    "        column_name = f'hk{j}_t{time_interval}Count'\n",
    "        train_data_new[column_name] = counts[base_index + j]\n",
    "    \n",
    "train_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb647",
   "metadata": {},
   "source": [
    "We then realised this was not very meaningful. How often a player presses a certain key, does not make him or her recognizable. So we decided to focus on frequency of moves. Meaning how frequent a player was using certain moves. Overall and again also over the course of certain time periods. To standardize the data we also created three new columns that mapped the name of the race (which was of type string before) to a zero or a one. Depending on whether this particular race was being played or not. \n",
    "Of course, preprocessing also included deleting unnecessary columns like Player URL and Player Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c25f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_moves(row, counts, index):\n",
    "    total_moves = 0\n",
    "    for i in range(1, 2564):\n",
    "        move = row[\"Move \"+ str(i)]\n",
    "        # count the number of s's\n",
    "        if move == 's':\n",
    "            counts[10][index] += 1\n",
    "        # count the number of Base's\n",
    "        elif move == 'Base':\n",
    "            counts[11][index] += 1\n",
    "        # count the number of SingleMineral's\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[12][index] += 1\n",
    "        # count the hotkeys\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[j][index] += 1\n",
    "\n",
    "        total_moves += 1  \n",
    "    # Save the total moves count\n",
    "    counts[13][index] = total_moves\n",
    "\n",
    "\n",
    "def count_move_per_time(row, counts, row_index, time_interval, ti_index):\n",
    "    base_index = ti_index * 14\n",
    "    total_moves = 0\n",
    "\n",
    "    for i in range(1, 2564):\n",
    "        move = row[\"Move \" + str(i)]\n",
    "\n",
    "        # Count actions for the given time interval\n",
    "        if move == 's':\n",
    "            counts[base_index + 10][row_index] += 1\n",
    "        elif move == 'Base':\n",
    "            counts[base_index + 11][row_index] += 1\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[base_index + 12][row_index] += 1\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[base_index + j][row_index] += 1\n",
    "\n",
    "        total_moves += 1\n",
    "\n",
    "        # Counting actions after the specified time interval\n",
    "        if move == f't{time_interval}':\n",
    "            break\n",
    "\n",
    "    counts[base_index + 13][row_index] = total_moves\n",
    "\n",
    "\n",
    "def mapRaces(races, row_index):\n",
    "    race = train_data['Race'][row_index]\n",
    "\n",
    "    if race == \"Protoss\":\n",
    "        races[0][row_index] = 1\n",
    "    elif race == \"Terran\":\n",
    "        races[1][row_index] = 1\n",
    "    elif race == \"Zerg\":\n",
    "        races[2][row_index] = 1\n",
    "        \n",
    "# Create new table that only contains the first column (PlayerId) of train_data\n",
    "# Keep only the first column but all rows\n",
    "train_data_new = train_data.iloc[:, :1]\n",
    "\n",
    "\n",
    "# Specify the target time intervals\n",
    "#time_intervals = [20, 60, 100, 200]\n",
    "time_intervals = [5, 20, 60, 100, 200, 270, 340, 550]\n",
    "\n",
    "calc_column = len(time_intervals)* 14 + 14\n",
    "\n",
    "# New lists of counts\n",
    "counts = [[0] * 3052 for _ in range(calc_column)]\n",
    "# New lists of races\n",
    "races = [[0] * 3052 for _ in range(3)]\n",
    "\n",
    "\n",
    "# Go through the rows using the functions to count the actions, map the races\n",
    "for row_index, row in train_data.iterrows():\n",
    "    count_moves(row, counts, row_index)\n",
    "    mapRaces(races, row_index)\n",
    "\n",
    "    for ti_index, time_interval in enumerate(time_intervals):\n",
    "        count_move_per_time(row, counts, row_index, time_interval, ti_index+1)\n",
    "        \n",
    "\n",
    "for i in range(calc_column):\n",
    "    locals()[f'count_{i}'] = counts[i]\n",
    "\n",
    "for i in range(10):\n",
    "    train_data_new[f'hk{i}Frequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[i])]\n",
    "\n",
    "train_data_new['sFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[10])]\n",
    "train_data_new['baseFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[11])]\n",
    "train_data_new['singleMineralFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[12])]\n",
    "\n",
    "# Adding new columns for the count of moves per interval\n",
    "for ti_index, time_interval in enumerate(time_intervals):\n",
    "    base_index = (ti_index + 1) * 14\n",
    "    for j in range(10):\n",
    "        column_name = f'hk{j}_t{time_interval}_Frequency'\n",
    "        train_data_new[column_name] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + j])]\n",
    "\n",
    "    train_data_new[f's_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 10])]\n",
    "    train_data_new[f'base_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 11])]\n",
    "    train_data_new[f'singleMineral_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 12])]\n",
    "\n",
    "\n",
    "\n",
    "# Adding new columns for the races\n",
    "train_data_new['race_Protoss'] = races[0]\n",
    "train_data_new['race_Terran'] = races[1]\n",
    "train_data_new['race_Zerg'] = races[2]\n",
    "\n",
    "# # Saving them in a csv file\n",
    "# train_data_new.to_csv('actiontype_count.csv', index=False)\n",
    "\n",
    "train_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3003b8",
   "metadata": {},
   "source": [
    "By analysing our train dataset, we realised that the same player uses typically the same sequence of moves, for a certain race type, in the first 10 seconds with slight variations. So we wanted to find a unique sequence for each player that immediately identified him by the use of a specific combination of moves, for a specific type of race played. \n",
    "We started working on this idea by creating a function for the research of consecutive sequences of moves, excluding the time data,'t5' and 't10' and iterate the process for each player to find the action sequences.\n",
    "At the end we saved the found sequences to a text file.\n",
    "We proceeded by defining a function to evaluate the combination of sequences based on their uniqueness, and applied to each player to obtain ranked combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df23963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices for columns 'Move_1' to 'Move_50'\n",
    "move_columns = [f'Move_{i}' for i in range(1, 68)]\n",
    "\n",
    "data_10s = []\n",
    "\n",
    "# Iterate through each row of the dataframe\n",
    "for _, row in train_data.iterrows():\n",
    "    row_actions = []\n",
    "\n",
    "    # Iterate through each 'Move_XX' column for the current row\n",
    "    for col in train_data.columns[3:70]:\n",
    "        row_actions.append(row[col])\n",
    "\n",
    "        # Check if the current action is 't10'\n",
    "        if row[col] == 't10':  # Assuming 't10' is converted to 100 in your previous processing\n",
    "            break  # Stop iterating if 't10' is found\n",
    "\n",
    "    data_10s.append(row_actions)\n",
    "\n",
    "# Convert the result to a new dataframe if needed\n",
    "data_10s_df = pd.DataFrame(data_10s, columns=move_columns)\n",
    "\n",
    "data_10s_df.insert(0, 'PlayerID', train_data['PlayerID'])\n",
    "data_10s_df.insert(1, 'Race', train_data['Race'])\n",
    "\n",
    "data_10s_df.to_csv('data_10s.csv', index=False)\n",
    "\n",
    "# Load the data_10s.csv file\n",
    "data_10s_df = pd.read_csv('data_10s.csv')\n",
    "\n",
    "# Get the move columns\n",
    "move_columns = [f'Move_{i}' for i in range(1, 68)]\n",
    "\n",
    "# Flatten the dataframe to have a single column of moves\n",
    "all_moves = data_10s_df[move_columns].values.flatten()\n",
    "\n",
    "# Count the frequency of each move\n",
    "moves_frequency = {}\n",
    "for move in all_moves:\n",
    "    if pd.notna(move):  # Exclude NaN values\n",
    "        moves_frequency[move] = moves_frequency.get(move, 0) + 1\n",
    "\n",
    "print(\"Moves Frequency:\")\n",
    "for move, frequency in moves_frequency.items():\n",
    "    print(f\"{move}: {frequency}\")\n",
    "\n",
    "# Group the data by PlayerID and reset the index\n",
    "grouped_data = data_10s_df.groupby('PlayerID')[move_columns].apply(lambda x: x.reset_index(drop=True))\n",
    "\n",
    "# Define a function to find sequences of consecutive moves\n",
    "def find_sequences(group):\n",
    "    sequences = []\n",
    "    for _, g in groupby(enumerate(group), key=lambda x: int(x[1] == 't10')):\n",
    "        consecutive_moves = list(map(lambda x: x[1], g))\n",
    "\n",
    "        # Remove 't5' and 't10' from consecutive_moves\n",
    "        consecutive_moves = [move for move in consecutive_moves if move not in ['t5', 't10']]\n",
    "\n",
    "        sequences.append(consecutive_moves)\n",
    "    return sequences\n",
    "\n",
    "# Iterate through each player's moves and find sequences\n",
    "player_sequences = {}\n",
    "for player, moves in grouped_data.iterrows():\n",
    "    sequences = find_sequences(moves.dropna().astype(str))\n",
    "    player_sequences[player] = sequences\n",
    "\n",
    "# Save the found sequences to a text file\n",
    "output_file_path = 'sequences.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for player, sequences in player_sequences.items():\n",
    "        file.write(f\"{player} : \\t\")\n",
    "        for sequence in sequences:\n",
    "            file.write(','.join(sequence) + '\\n')\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Sequences saved to {output_file_path}\")\n",
    "\n",
    "# Define a function to evaluate the combination of moves\n",
    "def evaluate_combination(combination):\n",
    "    unique_moves = set(combination[0] + combination[1])\n",
    "    uniqueness_score = sum(moves_frequency.get(move, 0) for move in unique_moves)\n",
    "    return uniqueness_score\n",
    "\n",
    "# Iterate through each player's sequences and find ranked combinations\n",
    "ranked_combinations = {}\n",
    "for player, sequences in player_sequences.items():\n",
    "    combinations_list = list(combinations(sequences, 2))\n",
    "    ranked_combinations[player] = sorted(combinations_list, key=lambda x: evaluate_combination(x), reverse=True)\n",
    "\n",
    "# Print or save the ranked combinations\n",
    "for player, combinations in ranked_combinations.items():\n",
    "    print(f\"Player {player} ranked combinations:\")\n",
    "    for i, combination in enumerate(combinations, start=1):\n",
    "        print(f\"Rank {i}: {combination} - Score: {evaluate_combination(combination)}\")\n",
    "\n",
    "# Save the ranked combinations to a text file\n",
    "output_file_path = 'ranked_combinations.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for player, combinations in ranked_combinations.items():\n",
    "        file.write(f\"Player {player} ranked combinations:\\n\")\n",
    "        for i, combination in enumerate(combinations, start=1):\n",
    "            file.write(f\"Rank {i}: {combination} - Score: {evaluate_combination(combination)}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Ranked combinations saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db880b8",
   "metadata": {},
   "source": [
    "At this point we stopped as we realised that we should create a training model that find the best sequence of actions for each player based on the ranking and then combine this model with others, as RandomForest, in order to predict in the most accurate way the player based also on the total time of the race and on other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742788d7",
   "metadata": {},
   "source": [
    "## Choosing a model\n",
    "\n",
    "Choosing the right model is crucial, because it directly influences the accuracy of the predictions and therefor impacts the overall success and reliability of our system. We will evaluate the different models we tried by using the F1 accuracy and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6806f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "labels = train_data_new['PlayerID']\n",
    "\n",
    "# Keep only the columns we need as features\n",
    "features = train_data_new.drop(['PlayerID'], axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505a988",
   "metadata": {},
   "source": [
    "We started by using the TreeClassifier. This gave us a training accuracy of 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05154e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Decision Tree as a model and train it\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the val set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_DT = f1_score(y_val, predictions, average='micro')\n",
    "print(f'F1 Score on Validation Set: {f1_DT}')\n",
    "\n",
    "scores = cross_val_score(model, features, labels, cv=4)\n",
    "print(f'Cross Validation Scores: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebfc98",
   "metadata": {},
   "source": [
    "With the RandomForest we were able to increase the training accuracy to 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35ec16f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Choose Random Forest as a model and train it\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predictions on the val set\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choose Random Forest as a model and train it\n",
    "model = RandomForestClassifier(random_state=42, n_estimators=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the val set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_DT = f1_score(y_val, predictions, average='micro')\n",
    "print(f'F1 Score on Validation Set: {f1_DT}')\n",
    "\n",
    "scores = cross_val_score(model, features, labels, cv=4)\n",
    "print(f'Cross Validation Scores: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8097d55",
   "metadata": {},
   "source": [
    "For the following changes we were not able to increase our training accuracy. In return they improved our testing accuracy on kaggle and therefore also our model. Firstly we performed hyperparameter tuning using GridSearchCV on the RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6198b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model and train it\n",
    "model = RandomForestClassifier(random_state=42, n_estimators=200)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 150, 200], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=4, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Choose a model and train it\n",
    "best_model = RandomForestClassifier(random_state=42, n_estimators=500)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the best model for predictions\n",
    "predictions = best_model.predict(X_val)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_DT = f1_score(y_val, predictions, average='micro')\n",
    "print(f'F1 Score on Validation Set: {f1_DT}')\n",
    "\n",
    "scores = cross_val_score(best_model, features, labels, cv=4)\n",
    "print(f'Cross Validation Scores: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81951d2c",
   "metadata": {},
   "source": [
    "Then we tested our model and plotted which features contributed the most to the predictions. \n",
    "[You can find the results here](Feature_Importance.ipynb). We did this with the help of RandomForest feature importace. It is a measure of the contribution of each feature to the overall predictive performance of the model.  \n",
    "We then removed 30% of the least important features. The F1 score stayed the same for the training and testing, so to reduce dimensions we decided to stick to the smaller table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f8e3e-cb97-4829-94f3-88fefa1d6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 30% of least important features\n",
    "columns_to_remove = ['hk1_t5_Frequency', 'race_Zerg', 'hk9_t60_Frequency', 'hk5_t5_Frequency', 'hk7_t60_Frequency', 'hk7_t550_Frequency', 'hk9_t340_Frequency', 'hk0_t20_Frequency', 'hk6_t20_Frequency', 'base_t20_Frequency', 'hk8_t200_Frequency', 'hk7_t270_Frequency', 'hk9_t20_Frequency', 'base_t5_Frequency', 'hk9_t200_Frequency', 'hk7_t340_Frequency', 'singleMineral_t550_Frequency', 'singleMineral_t200_Frequency', 'singleMineral_t340_Frequency', 'hk9_t270_Frequency', 'sFrequency', 'hk8_t100_Frequency', 'hk0_t5_Frequency', 'race_Terran', 'singleMineralFrequency', 'hk7_t20_Frequency', 'singleMineral_t270_Frequency', 'singleMineral_t100_Frequency', 'hk8_t60_Frequency', 'hk8_t20_Frequency', 'singleMineral_t60_Frequency', 'hk6_t5_Frequency', 'hk7_t5_Frequency', 'hk8_t5_Frequency', 'singleMineral_t20_Frequency', 'singleMineral_t5_Frequency']\n",
    "# Remove columns from DataFrame\n",
    "train_data_ne = train_data_new.drop(columns=columns_to_remove)\n",
    "\n",
    "# # Saving them in a csv file\n",
    "# train_data_ne.to_csv('actiontype_count.csv', index=False)\n",
    "\n",
    "train_data_ne.head()\n",
    "\n",
    "# Target\n",
    "labels = train_data_ne['PlayerID']\n",
    "\n",
    "# Keep only the columns we need as features\n",
    "features = train_data_ne.drop(['PlayerID'], axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Choose a model and train it\n",
    "model = RandomForestClassifier(random_state=42, n_estimators=500)\n",
    "\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 150, 200], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=4, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model_cut = grid_search.best_estimator_\n",
    "\n",
    "# Choose a model and train it\n",
    "best_model_cut = RandomForestClassifier(random_state=42, n_estimators=500)\n",
    "best_model_cut.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model_cut, 'player_id_prediction_model.pkl')\n",
    "\n",
    "\n",
    "# Use the best model for predictions\n",
    "predictions = best_model_cut.predict(X_val)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_DT = f1_score(y_val, predictions, average='micro')\n",
    "print(f'F1 Score on Validation Set: {f1_DT}')\n",
    "\n",
    "#scores = cross_val_score(best_model, features, labels, cv=4)\n",
    "#print(f'Cross Validation Scores: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a4fd3",
   "metadata": {},
   "source": [
    "To increase the testing accuracy even further we added the AdaBoost method. It focuses on instances that are difficult to classify and thus complemented the RandomForest method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ad0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "labels = train_data_new['PlayerID']\n",
    "\n",
    "# Keep only the columns we need as features\n",
    "features = train_data_new.drop(['PlayerID'], axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Choose the boosting algorithm (AdaBoost)\n",
    "boosting_model = AdaBoostClassifier(best_model)\n",
    "\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    " # Save the best model to a file\n",
    "joblib.dump(boosting_model, 'player_id_prediction_model.pkl')\n",
    "\n",
    " # Use the best model for predictions\n",
    "predictions = boosting_model.predict(X_val)\n",
    "\n",
    " # Evaluation of model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_DT = f1_score(y_val, predictions, average='micro')\n",
    "print(f'F1 Score on Validation Set: {f1_DT}')\n",
    "\n",
    "#scores = cross_val_score(boosting_model, features, labels, cv=4)\n",
    "#print(f'Cross Validation Scores: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afe22f",
   "metadata": {},
   "source": [
    "### Visualisation of data\n",
    "Down below we show how we visualized our data and training algorithm. \n",
    "\n",
    "Feature Importance chart: To visualize the importance of the different features (moves/hotkeys/s/Base), we use a horizontal bar plot. The x-axis represents the importance scores and the y-axis shows the feature names. We showcase the 30 most important features below. \n",
    "\n",
    "Correlation heatmap: This matrix shows the pairwise correlations between the features. Some features don't correlate at all, while others have a strong negative or positive correlation. Based on the 30 most top features. \n",
    " \n",
    "Target classes distribution: With a bar chart we show the distributions between the different classes, 'playerID'. As we can see in model, some players have played a lot of races while others aren't represented as much. \n",
    "\n",
    "Confusion matrix: This matrix is used to evaluate the performance of the model. It breaks down how well the model is performing in regards of predictions, true/false negative/positive. On the diagonal we see the true (correct) predictions. The cells below and above the diagonal show false predictions made by the model. The labels [0, 49] represent the 50 top players. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': boosting_model.feature_importances_})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df, palette=\"crest\")\n",
    "plt.title(\"Feature Importance\", fontsize=12)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.xlabel(\"Importance\", fontsize=9)\n",
    "plt.ylabel(\"Feature\", fontsize=9)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Displaying the top correlated features\n",
    "top_features = feature_importances_df.nlargest(15, 'Importance')['Feature']\n",
    "correlation_matrix = pd.DataFrame(X_train[top_features]).corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Top Features)\", fontsize=14)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Target Classes Distribution\n",
    "top_classes = y_train.value_counts().nlargest(30)\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_classes.plot(kind='bar', color=\"#98BF64\")\n",
    "plt.title(\"Distribution of Top 30 Target Classes\", fontsize=14)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.xlabel(\"PlayerID\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion Matrix (after model prediction)\n",
    "top_classes_cm = y_train.value_counts().nlargest(50)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val, predictions, labels=top_classes_cm.index)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "\n",
    "plt.title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(fontsize=8, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba92254",
   "metadata": {},
   "source": [
    "Furthermore we tried to use different ensemble methods: Stacking and Voting. With neither of them we achieved a higher score of accuracy so we decided to leave them out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee24938",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "When given the test data we were looking at almost the same pattern as the training data: Each row was a gameplay with a race and the moves made. Just the identifaction of the player was missing. That one was for us to predict and to do so we used our pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae15c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new data file \n",
    "test_data = pd.read_csv('test_data.csv', delimiter=';')\n",
    "test_data.columns = ['Race'] + [f'Move_{i}' for i in range(1, 3446)]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afbfd1",
   "metadata": {},
   "source": [
    "### Testing Pre-processing\n",
    "\n",
    "But before, we had to change the data in the same way we did with the training data. We did this, so they would be able to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same preprocessing as in train_def so that we have save structured data\n",
    "def count_moves(row, counts, index):\n",
    "    total_moves = 0\n",
    "    for i in range(1, 3446):\n",
    "        move = row[\"Move_\"+ str(i)]\n",
    "\n",
    "        # count the number of s's\n",
    "        if move == 's':\n",
    "            counts[10][index] += 1\n",
    "        # count the number of Base's\n",
    "        elif move == 'Base':\n",
    "            counts[11][index] += 1\n",
    "        # count the number of SingleMineral's\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[12][index] += 1\n",
    "        # count the hotkeys\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[j][index] += 1\n",
    "        total_moves += 1  \n",
    "    # Save the total moves count\n",
    "    counts[13][index] = total_moves\n",
    "\n",
    "\n",
    "def count_move_per_time(row, counts, row_index, time_interval, ti_index):\n",
    "    base_index = ti_index*14\n",
    "    total_moves = 0\n",
    "    for i in range(1, 3446):\n",
    "        move = row[\"Move_\" + str(i)]\n",
    "\n",
    "        # Count actions for the given time interval\n",
    "        if move == 's':\n",
    "            counts[base_index + 10][row_index] += 1\n",
    "        elif move == 'Base':\n",
    "            counts[base_index + 11][row_index] += 1\n",
    "        elif move == 'SingleMineral':\n",
    "            counts[base_index + 12][row_index] += 1\n",
    "        elif isinstance(move, str):\n",
    "            for j in range(10):\n",
    "                if move.startswith(f\"hotkey{j}\"):\n",
    "                    counts[base_index + j][row_index] += 1\n",
    "\n",
    "        total_moves += 1\n",
    "\n",
    "        # Continue counting actions after the specified time interval\n",
    "        if move == f't{time_interval}':\n",
    "            break\n",
    "\n",
    "    counts[base_index + 13][row_index] = total_moves\n",
    "\n",
    "\n",
    "def mapRaces(races, row_index):\n",
    "    race = test_data['Race'][row_index]\n",
    "\n",
    "    if race == \"Protoss\":\n",
    "        races[0][row_index] = 1\n",
    "    elif race == \"Terran\":\n",
    "        races[1][row_index] = 1\n",
    "    elif race == \"Zerg\":\n",
    "        races[2][row_index] = 1\n",
    "        \n",
    "\n",
    "# Create new table that only contains the first column (Race) of test_data\n",
    "# Keep only the first column but all rows\n",
    "test_data_new = test_data.iloc[:, :1]\n",
    "\n",
    "# Specify the target time intervals\n",
    "# time_intervals = [20, 60, 100, 200]\n",
    "time_intervals = [5, 20, 60, 100, 200, 270, 340, 550]\n",
    "\n",
    "calc_column = len(time_intervals)* 14 + 14\n",
    "# New lists of counts\n",
    "counts = [[0] * 340 for _ in range(calc_column)]\n",
    "# New lists of races\n",
    "races = [[0] * 340 for _ in range(3)]\n",
    "\n",
    "\n",
    "# Go through the rows using the functions to count the actions, map the races\n",
    "for row_index, row in test_data.iterrows():\n",
    "    count_moves(row, counts, row_index)\n",
    "    mapRaces(races, row_index)\n",
    "\n",
    "    for ti_index, time_interval in enumerate(time_intervals):\n",
    "        count_move_per_time(row, counts, row_index, time_interval, ti_index+1)\n",
    "\n",
    "# Adding all the new columns to the test_data_new\n",
    "# Adding new columns for the count of moves\n",
    "for i in range(calc_column):\n",
    "    locals()[f'count_{i}'] = counts[i]\n",
    "\n",
    "for i in range(10):\n",
    "    test_data_new[f'hk{i}Frequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[i])]\n",
    "\n",
    "test_data_new['sFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[10])]\n",
    "test_data_new['baseFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[11])]\n",
    "test_data_new['singleMineralFrequency'] = [count / counts[13][index] if counts[13][index] != 0 else 0 for index, count in enumerate(counts[12])]\n",
    "\n",
    "# Adding new columns for the count of moves per interval\n",
    "for ti_index, time_interval in enumerate(time_intervals):\n",
    "    base_index = (ti_index + 1) * 14\n",
    "    for j in range(10):\n",
    "        column_name = f'hk{j}_t{time_interval}_Frequency'\n",
    "        test_data_new[column_name] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + j])]\n",
    "\n",
    "    test_data_new[f's_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 10])]\n",
    "    test_data_new[f'base_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 11])]\n",
    "    test_data_new[f'singleMineral_t{time_interval}_Frequency'] = [count / counts[base_index + 13][index] if counts[base_index + 13][index] != 0 else 0 for index, count in enumerate(counts[base_index + 12])]\n",
    "\n",
    "\n",
    "\n",
    "# Adding new columns for the races\n",
    "test_data_new['race_Protoss'] = races[0]\n",
    "test_data_new['race_Terran'] = races[1]\n",
    "test_data_new['race_Zerg'] = races[2]\n",
    "\n",
    "# # Remove 30% of least important features\n",
    "# columns_to_remove = ['hk1_t5_Frequency', 'race_Zerg', 'hk9_t60_Frequency', 'hk5_t5_Frequency', 'hk7_t60_Frequency', 'hk7_t550_Frequency', 'hk9_t340_Frequency', 'hk0_t20_Frequency', 'hk6_t20_Frequency', 'base_t20_Frequency', 'hk8_t200_Frequency', 'hk7_t270_Frequency', 'hk9_t20_Frequency', 'base_t5_Frequency', 'hk9_t200_Frequency', 'hk7_t340_Frequency', 'singleMineral_t550_Frequency', 'singleMineral_t200_Frequency', 'singleMineral_t340_Frequency', 'hk9_t270_Frequency', 'sFrequency', 'hk8_t100_Frequency', 'hk0_t5_Frequency', 'race_Terran', 'singleMineralFrequency', 'hk7_t20_Frequency', 'singleMineral_t270_Frequency', 'singleMineral_t100_Frequency', 'hk8_t60_Frequency', 'hk8_t20_Frequency', 'singleMineral_t60_Frequency', 'hk6_t5_Frequency', 'hk7_t5_Frequency', 'hk8_t5_Frequency', 'singleMineral_t20_Frequency', 'singleMineral_t5_Frequency']\n",
    "# # Remove columns from DataFrame\n",
    "# test_data_new = test_data_new.drop(columns=columns_to_remove)\n",
    "\n",
    "test_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f965a",
   "metadata": {},
   "source": [
    "### Testing Predictions \n",
    "\n",
    "The pre-trained model was then loaded and used to make predictions about the players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model_filename = 'player_id_prediction_model.pkl'\n",
    "clf = joblib.load(model_filename)\n",
    "\n",
    "features = test_data_new.drop(['Race'], axis=1)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "predictions = clf.predict(features)\n",
    "\n",
    "# Add predictions to the test_data_new DataFrame\n",
    "test_data_new['Predicted_PlayerID'] = predictions\n",
    "\n",
    "test_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf214bd4",
   "metadata": {},
   "source": [
    "At the end we created a .csv file with the results that we could upload to Kaggle. This meant merging the predicted player IDs with the first row of the training data to obtain the corresponding player URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b305e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv('train_data.csv', delimiter=';')\n",
    "\n",
    "# Extract 'PlayerID' and 'PlayerURL' columns\n",
    "player = train_data[['PlayerID', 'PlayerURL']]\n",
    "\n",
    "# take only one URL per each PlayerID\n",
    "player_info = player.drop_duplicates(subset='PlayerID', keep='first')\n",
    "\n",
    "# Extract 'Predicted_PlayerID' column\n",
    "player_id_column = test_data_new[['Predicted_PlayerID']]\n",
    "\n",
    "# Merge the predicted ID to get the url of the player\n",
    "result = pd.merge(player_id_column, player_info, left_on='Predicted_PlayerID', right_on='PlayerID', how='left')\n",
    "\n",
    "# take only the url for each player\n",
    "result = result.drop(['Predicted_PlayerID', 'PlayerID'], axis=1)\n",
    "# insert a new row in position 0 as asked for the submission and count the number of lines\n",
    "result.insert(0,\"RowId\", range(1, len(result) + 1))\n",
    "\n",
    "# Save 'PlayerURL' to CSV\n",
    "result.to_csv('player_id_only.csv', index=False)\n",
    "\n",
    "#player_info.head()\n",
    "#player_id_column.head()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e502050",
   "metadata": {},
   "source": [
    "Afterwards we added some statistics to asses the testing performance. We wanted to see which predictions were made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9575a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Predicted PlayerIDs:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount of Predicted PlayerIDs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_data_new\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_PlayerID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_new' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Count of Predicted PlayerIDs:\")\n",
    "print(test_data_new['Predicted_PlayerID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734eec7",
   "metadata": {},
   "source": [
    "And we wanted to see how many different players were predicted. We increased this score over time, because it meant our model was able differentiate better between players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa9e894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique PlayerURLs:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount of Unique PlayerURLs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayerURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Count of Unique PlayerURLs:\")\n",
    "print(result['PlayerURL'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79879abb",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The first time we tested our model we only reached 44% on Kaggle. Over time, we managed to increase this score more and more. Finally, we reached a level of 82% testing accuracy and 93% training accuracy. \n",
    "\n",
    "### Improvements\n",
    "We could make some improvements to our code by doing:\n",
    "- Feature Engineering:\n",
    "    We should consider additional features or transformations that might better capture the characteristics of our data and experiment with different aggregations, scaling, or encoding techniques. Another thing to do could be to check for highly correlated features and trying top remove the correlated features to reduce multicollinearity.\n",
    "- Model Selection:\n",
    "    Apart from the model already used, we could experiment with different machine learning models, that combines the results of other models, in order to obtain better results. Some examples could be Gradient Boosting, Support Vector Machines, or Neural Networks, which may capture complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
